---
tags: []
title: running_time
---
__FORCETOC__ In computer science, the *running time* of a function or
algorithm is a measure of its computational complexity. The running time
describes the amount of time the function or algorithm requires to
produce a result as a function of its input. The running time therefore
does not compute the exact amount of time a function takes to execute.
Instead, it expresses how fast the number of operations required to
return a result grows as we increase the input to some arbitrarily large
value.

In CS50, running time will generally be expressed in terms of Big-O or
asymptotic notation which measures the worst case running times of the
algorithm. We may also measure complexity through the best-case running
time (Ω) and the expected running time (Θ). The rest of this article
focuses on worst-case running times however much of the information is
equally relevant to best-case and expected running times. The
differences between the three notations are discussed in each of the
sub-sections.

[[]]
Asymptotic or Big O Notation
----------------------------

*Asymptotic notation* or *big O notation* is the mathematical way of
describing the running time of algorithms. The running time of an
algorithm is expressed as a function latexmath:[$f(n)$], where
latexmath:[$n$] generally refers to the number of elements that compose
the data structure on which the algorithm operates. A function that
operates on a string for example, would have latexmath:[$n$] equal to
the number of characters in the string whereas a function that operates
on an array of integers would have latexmath:[$n$] equal to the number
of integers in the array. The number of atomic operations an algorithm
performs will be some function of this input size latexmath:[$n$] and so
the time required for the algorithm to run will be proportional to
latexmath:[$f(n)$].

Because the purpose of the running time is not to predict the
performance of one algorithm, but to compare the performance of
alternative algorithms as latexmath:[$n$] grows arbitrarily large,
computer scientists focus on the order of the function
latexmath:[$f(n)$] rather than its particular structure. For example, if
we know that an algorithm needs time proportional to
latexmath:[$f(n) = 2*n^2+3n$], we say that it runs in “order
latexmath:[$n^2$] time”, written latexmath:[$O(n^2)$] in big O notation,
because it’s the latexmath:[$n^2$] term in latexmath:[$f(n)$] that
characterizes the algorithm’s performance for large n. The notation
latexmath:[$O(n^2)$] stands for the class of worst-case complexity
measures whose growth is proportional to latexmath:[$n^2$] when
latexmath:[$n$] is large.

We can say formally what it means for a function, latexmath:[$f(n)$] to
be in the class latexmath:[$O(g(n))$] like this:

::
  latexmath:[$f(n) \in O(g(n)) \mbox{ if and only if} $]
  +
  latexmath:[$ \mbox{ there exist constants } c > 0 \mbox{ and } n_0 \ge 1 \mbox{ such that } $]
  +
  latexmath:[$f(n) \leq cg(n) \mbox{ for all } n > n_0$]

In English: “a function latexmath:[$f(n)$] is in the class of functions
of order latexmath:[$g(n)$] if and only if latexmath:[$cg(n)$]
eventually surpasses latexmath:[$f(n)$] when latexmath:[$n$] becomes
large enough. Focusing on the order of the function allows us to isolate
a small number of classes of running times.

[[]]
Constant (latexmath:[$O(1)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A constant running time, represented by the Big-O notation
latexmath:[$O(1)$], is the most preferable running time because the
maximum number of operations required to produce a result remains
constant as the input size grows. A constant running time described by
any function with a leading term of the form latexmath:[$f(n) = a$]
where a is a real number. Constant running times are very rare except
for the most rudimentary programs such as determining whether a number
is even or odd.

[[]]
Logarithmic (latexmath:[$O(\log n)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Logarithmic running time, represented by the Big-O notation
latexmath:[$O(\log n)$], applies to algorithms that compute a result
proportional to the base 2 logarithm of the input, otherwise expressed
as the function latexmath:[$f(n) = \log_2 n$]. A base of two is
generally assumed in computer science so the base is often omitted when
writing the Big-O notation for logarithmic running times. The base of
the logarithm is not essential however since any logarithm with another
base would be proportional to a base 2 logarithm by a constant factors
and such factors are discarded in Big-O notation. Like
latexmath:[$O(1)$] running times, latexmath:[$O(\log n)$] running times
represent a very low level of complexity. Algorithms which split their
input in half and operates successively on one of the halves such as
binary search usually exhibit logarithmic running time.

[[]]
Linear (latexmath:[$O(n)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Linear or latexmath:[$O(n)$] running times describes an algorithm in
which the maximum number of operations required to produce a result is
related to the input size by a constant factor. Although linear time is
often desirable, especially for more developed algorithms, some
algorithms that run in linear time can be improved to run in logarithmic
time. For example linear search, which traverses all the elements of an
array in the worst case, can be improved upon by binary search. Of
course binary search required sorted lists whereas linear search accepts
unsorted lists as well.

[[]]
Linearithmic (latexmath:[$O(n\log n)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Linearithmic or quasilinear running time or latexmath:[$O(n\log n)$]
signifies that the maximum number of procedures required for completion
is proportional to the product of the input size and its base two
logarithm. Linearithmic running times often describe functions that
combine linear and logarithmic algorithms such as mergesort. In merge
sort, splitting the list repeatedly requires logarithmic time whereas
comparing the elements during the merging process takes linear time.

[[]]
Polynomial (latexmath:[$O(n^c)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

o Polynomial or latexmath:[$O(n^c)$] running time is the slowest of the
desirable running times. The most common polynomial running time is
quadratic or latexmath:[$O(n^2)$] running time. Quadratic running times
often occurs in algorithms that perform binary operations confronting
each element of the input against all the others such as insertions sort
and bubble sort. Algorithms that rely on nested for loops often exhibit
polynomial running time.

[[]]
Exponential (latexmath:[$O(c^n)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Exponential or latexmath:[$O(c^n)$] running time is usually undesirable
because it represents a program with a complexity that is multiplied by
a constant factor with each added input element.

[[]]
Factorial (latexmath:[$O(n!)$])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Factorial or latexmath:[$O(n!)$] running time is highly undesirable and
usually reflects misguided algorithm design.

[[]]
Differences between O, latexmath:[$\Omega$], and latexmath:[$\Theta$]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Whereas O describes the worst-case running time of an algorithm,
latexmath:[$\Omega$] and latexmath:[$\Theta$] describe the best-case and
average-case running times of an algorithm, respectively. However, the
classifications above, e.g. linear, exponential, polynomial, apply to
latexmath:[$\Omega$] and latexmath:[$\Theta$] as well as to O. It's
important to note, though, that an algorithm can be in one
classification for O and entirely different classifications for
latexmath:[$\Omega$] and latexmath:[$\Theta$].

The difference between O, latexmath:[$\Omega$], and latexmath:[$\Theta$]
lies in the initial assumptions that are made. In the case of sorting,
for example, the assumption we make for the worst-case scenario is that
the data being sorted is in the exact opposite order of what we want.
For example, if our goal is to sort an array of integers from smallest
to largest, then the worst-case scenario would be if the integers were
given to us in order of largest to smallest. It follows, then, that the
best-case scenario would be if the integers were given to us already
sorted in order of smallest to largest.

Assuming that we are given an already-sorted array in the best-case
scenario, you might think that every sorting algorithm would be in
latexmath:[$\Omega(1)$]: the algorithm looks at the array in one step,
sees that it's sorted, and returns. However, remember that the algorithm
can't actually examine the whole array in one step. It would need at
least latexmath:[$n$] steps in order to examine all latexmath:[$n$]
elements in the array. Even then, not every sorting algorithm is in
latexmath:[$\Omega(n)$]. Some sorting algorithms have no checks in place
to know if they can stop sorting early. Consider link:Bubble Sort[Bubble
Sort], for example, in which elements are compared two at a time and
swapped if they are found to be out of order. This algorithm is in
O(latexmath:[$n^2$]) because if the array is completely out of order, it
will take latexmath:[$n$] swaps to move the largest element from the
start of the array to the end of the array. Each of these
latexmath:[$n$] swaps occurs on one walkthrough of the array, which
takes latexmath:[$n$] steps. Thus latexmath:[$n * n$].

Now, considering the best-case scenario with link:Bubble Sort[Bubble
Sort], that is, if the array is already sorted, how can we tell that
nothing needs to be done and we can return? If on the first walkthrough
of the array, no swaps are made, then the array must be already sorted.
But if we don't check whether any swaps are made on each walkthrough of
the array, then our algorithm will make latexmath:[$n$] walkthroughs
regardless of when the array is done being sorted. Adding this simple
check into our algorithm moves it from latexmath:[$\Omega(n^2)$] to
latexmath:[$\Omega(n)$].

While link:Bubble Sort[Bubble Sort] can be optimized in this way to move
it to latexmath:[$\Omega(n)$], link:Selection Sort[Selection Sort]
cannot. On any given walkthough using link:Selection Sort[Selection
Sort], the computer has no way of knowing that the smallest number is
already in the correct position, so it must complete latexmath:[$n$]
walkthroughs of the array in order to guarantee that it is sorted.

[[]]
Determining Efficiency
----------------------

In CS50, it is sufficient to determine the running time of an algorithm
by approximating the number of step required. In general, the most
important factor in determining the efficiency of an algorithm will be
the loops it employs. The following examples are a good guide.

Example 1:

code,c------------------------------------------- code,c
for(i=0; i<n; i++)
{
    // loop body that runs in constant time
}
-------------------------------------------

Example 2:

code,c----------------------------------------------- code,c
for(i=0; i<n; i++)
{
    for(j=0; j<n; j++)
    {
        // loop body that runs in constant time
    }
}
-----------------------------------------------

Because example 1 iterates once through the loop for each
latexmath:[$n$], it runs in linear of latexmath:[$O(n)$] time. Example
2, on the other hand, runs in polynomial or latexmath:[$O(n^2)$] time
because the algorithm goes through latexmath:[$n$] iterations of the
inner loop for each latexmath:[$n$]. That makes for a total of
latexmath:[$n*n=n^2$]. More advanced computer science courses will
employ more sophisticated methods to detmerne a program's running time
but they are beyond the scope of this course.

[[]]
Efficiency of Recursive Algorithms
----------------------------------

In the case of recursive algorithms, determining the efficiency becomes
more complicated. Still, a few basic rules can facilitate the process:

::
  ;;
    A searching algorithm that reduces its data set by a constant amount
    (unrelated to n) and then calls itself recursively has
    latexmath:[$O(n)$] time behavior. Examples of an algorithm with this
    order include linear search, and searching through a linked list.
    +
    A searching algorithm that divides its data set in half and then
    operates recursively on only one half of the data set complexity
    latexmath:[$O(\log n)$]. Examples of an algorithm with this order
    are binary search, or searching through a balanced binary tree.
    +
    An algorithm that divides its data set in half, operates recursively
    on both halves, and then combines the two halves in an
    latexmath:[$O(n)$] operation, runs in latexmath:[$O(n \log n)$]
    time. Quicksort and merge sort (see the next section) are examples
    of this.
    +
    An algorithm that chooses an element in the data set and then
    performs some function of that element for each item in the rest of
    the data set, (such as comparing it to all the other elements in the
    set) before recursing (or looping) has latexmath:[$O(n^2)$]
    complexity. Selection sort and bubble sort are examples, although
    they’re usually expressed iteratively rather than recursively.

Category: Week 3[Category: Week 3]
